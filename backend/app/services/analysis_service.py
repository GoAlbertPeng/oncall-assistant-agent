"""Analysis service for alert analysis workflow with streaming support."""
import asyncio
import json
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any, AsyncGenerator, Set
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, desc
from app.models.session import AnalysisSession, AnalysisStatus
from app.models.datasource import DataSource, DataSourceType
from app.schemas.analysis import (
    AnalysisRequest, 
    ContextData, 
    AnalysisResult,
    IntentResult,
    StreamEvent,
)
from app.services import datasource_service
from app.services import llm_service
from app.services import test_data_service

# Track active analysis sessions that can be cancelled
_active_sessions: Set[int] = set()
_cancelled_sessions: Set[int] = set()


def is_session_cancelled(session_id: int) -> bool:
    """Check if a session has been cancelled."""
    return session_id in _cancelled_sessions


def cancel_session(session_id: int) -> bool:
    """Cancel an active session."""
    if session_id in _active_sessions:
        _cancelled_sessions.add(session_id)
        return True
    return False


async def create_analysis_session(
    db: AsyncSession,
    user_id: int,
    alert_content: str,
) -> AnalysisSession:
    """Create a new analysis session."""
    session = AnalysisSession(
        user_id=user_id,
        alert_content=alert_content,
        status="pending",
        messages=[],
    )
    db.add(session)
    await db.commit()
    await db.refresh(session)
    return session


async def get_session_by_id(
    db: AsyncSession,
    session_id: int,
) -> Optional[AnalysisSession]:
    """Get an analysis session by ID."""
    result = await db.execute(
        select(AnalysisSession).where(AnalysisSession.id == session_id)
    )
    return result.scalar_one_or_none()


async def list_sessions(
    db: AsyncSession,
    user_id: int,
    page: int = 1,
    page_size: int = 20,
) -> tuple[List[AnalysisSession], int]:
    """List analysis sessions for a user."""
    # Count total
    count_result = await db.execute(
        select(AnalysisSession).where(AnalysisSession.user_id == user_id)
    )
    total = len(count_result.scalars().all())
    
    # Get paginated results
    offset = (page - 1) * page_size
    result = await db.execute(
        select(AnalysisSession)
        .where(AnalysisSession.user_id == user_id)
        .order_by(desc(AnalysisSession.created_at))
        .offset(offset)
        .limit(page_size)
    )
    sessions = result.scalars().all()
    
    return sessions, total


async def update_session_status(
    db: AsyncSession,
    session: AnalysisSession,
    status: str,
    stage: str = None,
) -> None:
    """Update session status and stage."""
    session.status = status
    if stage:
        session.current_stage = stage
    await db.commit()


def _format_sse_event(event: StreamEvent) -> str:
    """Format a StreamEvent as SSE data."""
    data = event.model_dump(exclude_none=True)
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"


async def stream_analysis(
    db: AsyncSession,
    session: AnalysisSession,
    request: AnalysisRequest,
) -> AsyncGenerator[str, None]:
    """
    Stream analysis with multiple stages.
    
    Stages:
    1. Intent Understanding - Parse and understand the alert
    2. Data Collection - Collect logs and metrics
    3. LLM Analysis - Analyze with LLM
    """
    session_id = session.id
    _active_sessions.add(session_id)
    
    try:
        # ====== Stage 1: Intent Understanding ======
        yield _format_sse_event(StreamEvent(
            event="stage_start",
            stage="intent_understanding",
            content="ğŸ” æ­£åœ¨åˆ†æå‘Šè­¦æ„å›¾...",
        ))
        
        await update_session_status(db, session, "intent_understanding", "intent_understanding")
        
        # Check for cancellation
        if is_session_cancelled(session_id):
            yield _format_sse_event(StreamEvent(event="cancelled", content="åˆ†æå·²å–æ¶ˆ"))
            return
        
        # Parse intent
        intent = await _understand_intent(request.alert_content)
        session.intent = intent.model_dump()
        session.add_message("assistant", f"ğŸ“‹ **å‘Šè­¦æ‘˜è¦**: {intent.summary}", "intent_understanding")
        session.add_message("assistant", f"ğŸ·ï¸ **å‘Šè­¦ç±»å‹**: {intent.alert_type}", "intent_understanding")
        if intent.affected_system:
            session.add_message("assistant", f"ğŸ’» **å½±å“ç³»ç»Ÿ**: {intent.affected_system}", "intent_understanding")
        session.add_message("assistant", f"ğŸ”‘ **å…³é”®è¯**: {', '.join(intent.keywords)}", "intent_understanding")
        await db.commit()
        
        yield _format_sse_event(StreamEvent(
            event="stage_progress",
            stage="intent_understanding",
            content=f"å‘Šè­¦æ‘˜è¦: {intent.summary}",
            data=intent.model_dump(),
            progress=100,
        ))
        
        yield _format_sse_event(StreamEvent(
            event="stage_complete",
            stage="intent_understanding",
            content="âœ… æ„å›¾åˆ†æå®Œæˆ",
        ))
        
        await asyncio.sleep(0.3)  # Small delay for UI
        
        # ====== Stage 2: Data Collection ======
        if is_session_cancelled(session_id):
            yield _format_sse_event(StreamEvent(event="cancelled", content="åˆ†æå·²å–æ¶ˆ"))
            return
        
        yield _format_sse_event(StreamEvent(
            event="stage_start",
            stage="data_collection",
            content="ğŸ“Š æ­£åœ¨æ”¶é›†ç›¸å…³æ•°æ®...",
        ))
        
        await update_session_status(db, session, "data_collection", "data_collection")
        
        # Collect context with progress updates
        context = ContextData()
        datasources = await _get_datasources(db, request.datasource_ids)
        
        if not datasources:
            session.add_message("assistant", "âš ï¸ æœªé…ç½®æ•°æ®æºï¼Œå°†ä½¿ç”¨æµ‹è¯•æ•°æ®", "data_collection")
            yield _format_sse_event(StreamEvent(
                event="stage_progress",
                stage="data_collection",
                content="æœªé…ç½®æ•°æ®æºï¼Œæ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®...",
                progress=50,
            ))
            
            # Load test data when no datasources configured
            keywords = " ".join(intent.keywords) if intent.keywords else ""
            test_logs = await test_data_service.get_test_logs(query=keywords, limit=50)
            test_metrics = await test_data_service.get_test_metrics(limit=20)
            
            if test_logs:
                context.logs.extend([
                    {
                        "timestamp": log.get("timestamp", ""),
                        "level": log.get("level", "INFO"),
                        "message": log.get("message", ""),
                        "source": f"æµ‹è¯•æ•°æ®: {log.get('source', '')}",
                    }
                    for log in test_logs
                ])
                context.collection_status["test_logs"] = f"ä»æµ‹è¯•æ•°æ®æ”¶é›†åˆ° {len(test_logs)} æ¡æ—¥å¿—"
            
            if test_metrics:
                for m in test_metrics:
                    context.metrics.append({
                        "metric_name": m.get("name", "unknown"),
                        "labels": m.get("labels", {}),
                        "values": [{"timestamp": m.get("timestamp", ""), "value": m.get("value", 0)}],
                    })
                context.collection_status["test_metrics"] = f"ä»æµ‹è¯•æ•°æ®æ”¶é›†åˆ° {len(test_metrics)} æ¡æŒ‡æ ‡"
            
            session.add_message("assistant", f"ğŸ“¥ ä»æµ‹è¯•æ•°æ®æ”¶é›†åˆ° {len(test_logs)} æ¡æ—¥å¿—, {len(test_metrics)} æ¡æŒ‡æ ‡", "data_collection")
        else:
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(minutes=request.time_range_minutes)
            start_iso = start_time.isoformat() + "Z"
            end_iso = end_time.isoformat() + "Z"
            
            keywords = " ".join(intent.keywords) if intent.keywords else _extract_keywords(request.alert_content)
            
            total_ds = len(datasources)
            for i, ds in enumerate(datasources):
                if is_session_cancelled(session_id):
                    yield _format_sse_event(StreamEvent(event="cancelled", content="åˆ†æå·²å–æ¶ˆ"))
                    return
                
                yield _format_sse_event(StreamEvent(
                    event="stage_progress",
                    stage="data_collection",
                    content=f"æ­£åœ¨æŸ¥è¯¢æ•°æ®æº: {ds.name}",
                    progress=int((i / total_ds) * 100),
                ))
                
                await _collect_from_datasource(ds, keywords, start_iso, end_iso, context)
                session.add_message(
                    "assistant", 
                    f"ğŸ“¥ ä» **{ds.name}** æ”¶é›†åˆ° {context.collection_status.get(f'ds_{ds.id}', '0 æ¡æ•°æ®')}", 
                    "data_collection"
                )
                await db.commit()
        
        # Save context
        session.context_data = context.model_dump()
        await db.commit()
        
        yield _format_sse_event(StreamEvent(
            event="stage_complete",
            stage="data_collection",
            content=f"âœ… æ•°æ®æ”¶é›†å®Œæˆ: {len(context.logs)} æ¡æ—¥å¿—, {len(context.metrics)} æ¡æŒ‡æ ‡",
            data={"logs_count": len(context.logs), "metrics_count": len(context.metrics)},
        ))
        
        await asyncio.sleep(0.3)
        
        # ====== Stage 3: LLM Analysis ======
        if is_session_cancelled(session_id):
            yield _format_sse_event(StreamEvent(event="cancelled", content="åˆ†æå·²å–æ¶ˆ"))
            return
        
        yield _format_sse_event(StreamEvent(
            event="stage_start",
            stage="llm_analysis",
            content="ğŸ¤– æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œåˆ†æ...",
        ))
        
        await update_session_status(db, session, "llm_analysis", "llm_analysis")
        session.add_message("assistant", "ğŸ¤– æ­£åœ¨åˆ†æå‘Šè­¦åŸå› ï¼Œè¯·ç¨å€™...", "llm_analysis")
        await db.commit()
        
        # Stream LLM analysis
        yield _format_sse_event(StreamEvent(
            event="stage_progress",
            stage="llm_analysis",
            content="å¤§æ¨¡å‹æ­£åœ¨æ€è€ƒä¸­...",
            progress=30,
        ))
        
        result = await llm_service.analyze_alert(
            request.alert_content,
            context.logs,
            context.metrics,
        )
        
        if is_session_cancelled(session_id):
            yield _format_sse_event(StreamEvent(event="cancelled", content="åˆ†æå·²å–æ¶ˆ"))
            return
        
        # Save and stream result
        if result:
            session.analysis_result = result.model_dump()
            
            # Stream each part of the result
            yield _format_sse_event(StreamEvent(
                event="message",
                stage="llm_analysis",
                content=f"## ğŸ¯ æ ¹å› åˆ†æ\n{result.root_cause}",
            ))
            await asyncio.sleep(0.2)
            
            yield _format_sse_event(StreamEvent(
                event="message",
                stage="llm_analysis",
                content=f"## ğŸ“‹ è¯æ®\n{result.evidence}",
            ))
            await asyncio.sleep(0.2)
            
            yield _format_sse_event(StreamEvent(
                event="message",
                stage="llm_analysis",
                content=f"## ğŸ·ï¸ é—®é¢˜åˆ†ç±»\n{_translate_category(result.category)}",
            ))
            await asyncio.sleep(0.2)
            
            yield _format_sse_event(StreamEvent(
                event="message",
                stage="llm_analysis",
                content=f"## ğŸš‘ ä¸´æ—¶è§£å†³æ–¹æ¡ˆ\n{result.temporary_solution}",
            ))
            await asyncio.sleep(0.2)
            
            yield _format_sse_event(StreamEvent(
                event="message",
                stage="llm_analysis",
                content=f"## ğŸ”§ æ ¹æœ¬è§£å†³æ–¹æ¡ˆ\n{result.permanent_solution}",
            ))
            await asyncio.sleep(0.2)
            
            yield _format_sse_event(StreamEvent(
                event="message",
                stage="llm_analysis",
                content=f"## ğŸ“Š ç½®ä¿¡åº¦\n{int(result.confidence * 100)}%",
            ))
            
            # Add to messages
            session.add_message("assistant", f"**æ ¹å› åˆ†æ**: {result.root_cause}", "llm_analysis", result.model_dump())
        
        await db.commit()
        
        yield _format_sse_event(StreamEvent(
            event="stage_complete",
            stage="llm_analysis",
            content="âœ… åˆ†æå®Œæˆ",
        ))
        
        # ====== Complete ======
        await update_session_status(db, session, "completed")
        session.add_message("assistant", "åˆ†æå®Œæˆï¼æ‚¨å¯ä»¥ç»§ç»­æé—®æˆ–è¯·æ±‚è¿›ä¸€æ­¥åˆ†æã€‚", "completed")
        await db.commit()
        
        yield _format_sse_event(StreamEvent(
            event="done",
            content="åˆ†æå®Œæˆ",
            data={"session_id": session.id},
        ))
        
    except Exception as e:
        await update_session_status(db, session, "error")
        session.add_message("system", f"åˆ†æå‡ºé”™: {str(e)}", "error")
        await db.commit()
        
        yield _format_sse_event(StreamEvent(
            event="error",
            content=f"åˆ†æå‡ºé”™: {str(e)}",
        ))
    
    finally:
        _active_sessions.discard(session_id)
        _cancelled_sessions.discard(session_id)


async def _understand_intent(alert_content: str) -> IntentResult:
    """Parse and understand the alert intent."""
    # Simple intent parsing - could be enhanced with LLM
    content_lower = alert_content.lower()
    
    # Determine alert type
    if any(k in content_lower for k in ["cpu", "å†…å­˜", "memory", "disk", "ç£ç›˜", "load"]):
        alert_type = "performance"
    elif any(k in content_lower for k in ["error", "é”™è¯¯", "exception", "å¼‚å¸¸", "fail"]):
        alert_type = "error"
    elif any(k in content_lower for k in ["down", "å®•æœº", "unreachable", "è¶…æ—¶", "timeout"]):
        alert_type = "availability"
    elif any(k in content_lower for k in ["network", "ç½‘ç»œ", "connection", "è¿æ¥"]):
        alert_type = "network"
    else:
        alert_type = "general"
    
    # Extract keywords
    keywords = _extract_keywords(alert_content).split()
    
    # Extract affected system
    affected_system = None
    for word in alert_content.split():
        if word.endswith("-service") or word.endswith("æœåŠ¡"):
            affected_system = word
            break
    
    # Suggest metrics based on type
    suggested_metrics = []
    if alert_type == "performance":
        suggested_metrics = ["cpu_usage", "memory_usage", "disk_usage"]
    elif alert_type == "availability":
        suggested_metrics = ["up", "response_time", "error_rate"]
    elif alert_type == "network":
        suggested_metrics = ["network_in", "network_out", "connection_count"]
    
    return IntentResult(
        summary=alert_content[:100] if len(alert_content) > 100 else alert_content,
        alert_type=alert_type,
        affected_system=affected_system,
        keywords=keywords[:10],
        suggested_metrics=suggested_metrics,
    )


def _translate_category(category: str) -> str:
    """Translate category to Chinese."""
    translations = {
        "code_issue": "ä»£ç é—®é¢˜",
        "config_issue": "é…ç½®é—®é¢˜", 
        "resource_bottleneck": "èµ„æºç“¶é¢ˆ",
        "dependency_failure": "ä¾èµ–æ•…éšœ",
    }
    return translations.get(category, category)


async def _get_datasources(db: AsyncSession, datasource_ids: Optional[List[int]]) -> List[DataSource]:
    """Get datasources to query."""
    if datasource_ids:
        datasources = []
        for ds_id in datasource_ids:
            ds = await datasource_service.get_datasource_by_id(db, ds_id)
            if ds:
                datasources.append(ds)
        return datasources
    return await datasource_service.get_all_datasources(db)


def _extract_keywords(alert_content: str) -> str:
    """Extract search keywords from alert content."""
    stop_words = {"çš„", "æ˜¯", "åœ¨", "äº†", "å’Œ", "ä¸", "æˆ–", "a", "the", "is", "at", "for", "to", "and", "or"}
    words = alert_content.replace("ï¼Œ", " ").replace("ã€‚", " ").replace(",", " ").replace(".", " ").split()
    keywords = [w for w in words if w.lower() not in stop_words and len(w) > 1]
    return " ".join(keywords[:10])


async def _collect_from_datasource(
    datasource: DataSource,
    query_str: str,
    start_time: str,
    end_time: str,
    context: ContextData,
) -> None:
    """Collect data from a single datasource."""
    try:
        connector = datasource_service.get_connector(datasource)
        
        if datasource.type in [DataSourceType.ELK, DataSourceType.LOKI]:
            logs = await connector.query(query_str, start_time, end_time)
            
            # If no logs found from datasource, try test data
            if not logs:
                test_logs = await test_data_service.get_test_logs(query=query_str, limit=50)
                if test_logs:
                    logs = test_logs
                    context.collection_status[f"ds_{datasource.id}"] = f"ä»æµ‹è¯•æ•°æ®æ”¶é›†åˆ° {len(logs)} æ¡æ—¥å¿—"
                else:
                    context.collection_status[f"ds_{datasource.id}"] = "æœªæ‰¾åˆ°ç›¸å…³æ—¥å¿—"
            else:
                context.collection_status[f"ds_{datasource.id}"] = f"æ”¶é›†åˆ° {len(logs)} æ¡æ—¥å¿—"
            
            context.logs.extend([
                {
                    "timestamp": log.get("timestamp", ""),
                    "level": log.get("level", "INFO"),
                    "message": log.get("message", ""),
                    "source": f"{datasource.name}: {log.get('source', '')}",
                }
                for log in logs
            ])
        
        elif datasource.type == DataSourceType.PROMETHEUS:
            metric_queries = _get_prometheus_queries(query_str)
            all_metrics = []
            for query in metric_queries:
                try:
                    metrics = await connector.query(query, start_time, end_time)
                    all_metrics.extend(metrics)
                except Exception:
                    pass
            
            # If no metrics found, try test data
            if not all_metrics:
                test_metrics = await test_data_service.get_test_metrics(limit=20)
                if test_metrics:
                    # Convert test metrics to expected format
                    for m in test_metrics:
                        all_metrics.append({
                            "metric_name": m.get("name", "unknown"),
                            "labels": m.get("labels", {}),
                            "values": [{"timestamp": m.get("timestamp", ""), "value": m.get("value", 0)}],
                        })
                    context.collection_status[f"ds_{datasource.id}"] = f"ä»æµ‹è¯•æ•°æ®æ”¶é›†åˆ° {len(all_metrics)} æ¡æŒ‡æ ‡"
                else:
                    context.collection_status[f"ds_{datasource.id}"] = "æœªæ‰¾åˆ°ç›¸å…³æŒ‡æ ‡"
            else:
                context.collection_status[f"ds_{datasource.id}"] = f"æ”¶é›†åˆ° {len(all_metrics)} æ¡æŒ‡æ ‡"
            
            context.metrics.extend(all_metrics)
    
    except Exception as e:
        # On error, try to get test data as fallback
        error_msg = str(e)
        if datasource.type in [DataSourceType.ELK, DataSourceType.LOKI]:
            test_logs = await test_data_service.get_test_logs(query=query_str, limit=50)
            if test_logs:
                context.logs.extend([
                    {
                        "timestamp": log.get("timestamp", ""),
                        "level": log.get("level", "INFO"),
                        "message": log.get("message", ""),
                        "source": f"æµ‹è¯•æ•°æ®: {log.get('source', '')}",
                    }
                    for log in test_logs
                ])
                context.collection_status[f"ds_{datasource.id}"] = f"æ•°æ®æºè¿æ¥å¤±è´¥ï¼Œä»æµ‹è¯•æ•°æ®æ”¶é›†åˆ° {len(test_logs)} æ¡æ—¥å¿—"
            else:
                context.collection_status[f"ds_{datasource.id}"] = f"è¿æ¥å¤±è´¥: {error_msg[:50]}"
        elif datasource.type == DataSourceType.PROMETHEUS:
            test_metrics = await test_data_service.get_test_metrics(limit=20)
            if test_metrics:
                for m in test_metrics:
                    context.metrics.append({
                        "metric_name": m.get("name", "unknown"),
                        "labels": m.get("labels", {}),
                        "values": [{"timestamp": m.get("timestamp", ""), "value": m.get("value", 0)}],
                    })
                context.collection_status[f"ds_{datasource.id}"] = f"æ•°æ®æºè¿æ¥å¤±è´¥ï¼Œä»æµ‹è¯•æ•°æ®æ”¶é›†åˆ° {len(test_metrics)} æ¡æŒ‡æ ‡"
            else:
                context.collection_status[f"ds_{datasource.id}"] = f"è¿æ¥å¤±è´¥: {error_msg[:50]}"


def _get_prometheus_queries(keywords: str) -> List[str]:
    """Generate Prometheus queries based on keywords."""
    queries = []
    keywords_lower = keywords.lower()
    
    if "cpu" in keywords_lower:
        queries.append('rate(node_cpu_seconds_total{mode!="idle"}[5m])')
    if "memory" in keywords_lower or "å†…å­˜" in keywords_lower:
        queries.append('node_memory_MemAvailable_bytes')
    if "disk" in keywords_lower or "ç£ç›˜" in keywords_lower:
        queries.append('node_filesystem_avail_bytes')
    if "network" in keywords_lower or "ç½‘ç»œ" in keywords_lower:
        queries.append('rate(node_network_receive_bytes_total[5m])')
    
    if not queries:
        queries = ["up"]
    
    return queries


# ====== Legacy non-streaming methods for backward compatibility ======

async def collect_context(
    db: AsyncSession,
    alert_content: str,
    time_range_minutes: int,
    datasource_ids: Optional[List[int]] = None,
) -> ContextData:
    """Collect context data from configured data sources."""
    context = ContextData()
    datasources = await _get_datasources(db, datasource_ids)
    keywords = _extract_keywords(alert_content)
    
    if datasources:
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(minutes=time_range_minutes)
        start_iso = start_time.isoformat() + "Z"
        end_iso = end_time.isoformat() + "Z"
        
        tasks = []
        for ds in datasources:
            tasks.append(_collect_from_datasource(ds, keywords, start_iso, end_iso, context))
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    # If no logs collected, get test logs
    if not context.logs:
        test_logs = await test_data_service.get_test_logs(query=keywords, limit=50)
        if test_logs:
            context.logs.extend([
                {
                    "timestamp": log.get("timestamp", ""),
                    "level": log.get("level", "INFO"),
                    "message": log.get("message", ""),
                    "source": f"æµ‹è¯•æ•°æ®: {log.get('source', '')}",
                }
                for log in test_logs
            ])
            context.collection_status["test_logs"] = f"ä»æµ‹è¯•æ•°æ®æ”¶é›†åˆ° {len(test_logs)} æ¡æ—¥å¿—"
    
    # If no metrics collected, get test metrics
    if not context.metrics:
        test_metrics = await test_data_service.get_test_metrics(limit=20)
        if test_metrics:
            for m in test_metrics:
                context.metrics.append({
                    "metric_name": m.get("name", "unknown"),
                    "labels": m.get("labels", {}),
                    "values": [{"timestamp": m.get("timestamp", ""), "value": m.get("value", 0)}],
                })
            context.collection_status["test_metrics"] = f"ä»æµ‹è¯•æ•°æ®æ”¶é›†åˆ° {len(test_metrics)} æ¡æŒ‡æ ‡"
    
    if not context.logs and not context.metrics:
        context.collection_status["global"] = "æœªèƒ½æ”¶é›†åˆ°ä»»ä½•æ•°æ®"
    
    return context


async def perform_analysis(
    db: AsyncSession,
    user_id: int,
    request: AnalysisRequest,
) -> AnalysisSession:
    """Perform full analysis workflow (non-streaming)."""
    session = await create_analysis_session(db, user_id, request.alert_content)
    session.add_message("user", request.alert_content)
    
    context = await collect_context(
        db,
        request.alert_content,
        request.time_range_minutes,
        request.datasource_ids,
    )
    session.context_data = context.model_dump()
    await db.commit()
    
    result = await llm_service.analyze_alert(
        request.alert_content,
        context.logs,
        context.metrics,
    )
    
    if result:
        session.analysis_result = result.model_dump()
        session.add_message("assistant", f"æ ¹å› åˆ†æ: {result.root_cause}", "llm_analysis", result.model_dump())
    
    session.status = "completed"
    await db.commit()
    await db.refresh(session)
    return session


async def continue_analysis(
    db: AsyncSession,
    session: AnalysisSession,
    user_message: str,
) -> AsyncGenerator[str, None]:
    """Continue analysis with a follow-up question."""
    session_id = session.id
    _active_sessions.add(session_id)
    
    try:
        session.add_message("user", user_message)
        await db.commit()
        
        yield _format_sse_event(StreamEvent(
            event="stage_start",
            stage="follow_up",
            content="ğŸ¤– æ­£åœ¨å¤„ç†æ‚¨çš„é—®é¢˜...",
        ))
        
        # Build context from previous analysis
        previous_result = session.analysis_result or {}
        context_summary = f"""
ä¹‹å‰çš„åˆ†æç»“æœ:
- æ ¹å› : {previous_result.get('root_cause', 'æœªçŸ¥')}
- è¯æ®: {previous_result.get('evidence', 'æ— ')}
- åˆ†ç±»: {previous_result.get('category', 'æœªçŸ¥')}

ç”¨æˆ·çš„è¿½é—®: {user_message}
"""
        
        # Call LLM for follow-up
        result = await llm_service.analyze_alert(
            context_summary,
            session.context_data.get("logs", []) if session.context_data else [],
            session.context_data.get("metrics", []) if session.context_data else [],
        )
        
        if is_session_cancelled(session_id):
            yield _format_sse_event(StreamEvent(event="cancelled", content="åˆ†æå·²å–æ¶ˆ"))
            return
        
        if result:
            yield _format_sse_event(StreamEvent(
                event="message",
                stage="follow_up",
                content=result.root_cause,
            ))
            
            session.add_message("assistant", result.root_cause, "follow_up", result.model_dump())
        
        await db.commit()
        
        yield _format_sse_event(StreamEvent(
            event="done",
            content="å›ç­”å®Œæˆ",
        ))
        
    except Exception as e:
        yield _format_sse_event(StreamEvent(
            event="error",
            content=f"å¤„ç†å‡ºé”™: {str(e)}",
        ))
    
    finally:
        _active_sessions.discard(session_id)
        _cancelled_sessions.discard(session_id)
